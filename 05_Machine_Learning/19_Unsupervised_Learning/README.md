# ğŸ” Module 19 - Unsupervised Learning Complete

**Â© Muhammad Ketsar Ali Abi Wahid**

---

## ğŸ“Œ Overview

Module ini mengajarkan **Unsupervised Learning** - teknik machine learning untuk menemukan pola dalam data **tanpa labels**. Anda akan mempelajari **Clustering**, **Dimensionality Reduction**, dan **Anomaly Detection**!

---

## ğŸ¯ Learning Objectives

Setelah menyelesaikan module ini, Anda akan mampu:

âœ… Memahami perbedaan Supervised vs Unsupervised Learning

âœ… Mengimplementasikan **K-Means Clustering**

âœ… Menggunakan **Hierarchical Clustering** & Dendrograms

âœ… Menerapkan **DBSCAN** untuk density-based clustering

âœ… Melakukan **PCA** (Principal Component Analysis) untuk dimensionality reduction

âœ… Menggunakan **t-SNE** untuk visualization

âœ… Mendeteksi **Anomalies** dengan Isolation Forest

âœ… Mengevaluasi clustering dengan metrics yang tepat

âœ… Memilih jumlah cluster optimal (Elbow Method, Silhouette)

---

## ğŸ¤” Supervised vs Unsupervised Learning

### **Supervised Learning (Modules 16, 17, 23):**
```
Data: X (features) + y (labels) âœ…
Goal: Learn X â†’ y mapping
Task: Predict labels for new data
Examples: Classification, Regression
```

### **Unsupervised Learning (Module 19):**
```
Data: X (features) only âŒ No labels!
Goal: Find hidden patterns/structure
Task: Group similar data, reduce dimensions
Examples: Clustering, PCA, Anomaly Detection
```

### **Analogi Sederhana:**

**Supervised = Belajar dengan Guru**
- Teacher gives you questions AND answers
- You learn patterns from examples
- Test: apply what you learned

**Unsupervised = Explorasi Sendiri**
- No teacher, no answers
- You find patterns yourself
- Discover hidden structures

---

## ğŸ¯ CLUSTERING

### **What is Clustering?**

**Definisi:** Mengelompokkan data points into groups (clusters) dimana:
- Points dalam cluster yang sama = **SIMILAR** ğŸŸ¢
- Points dalam cluster berbeda = **DIFFERENT** ğŸ”´

**Analogi:**
```
Imagine organizing your closet:
- Group 1: T-shirts ğŸ‘•
- Group 2: Pants ğŸ‘–
- Group 3: Shoes ğŸ‘Ÿ
- Group 4: Accessories ğŸ’

You don't have labels, just naturally group similar items!
```

### **Use Cases:**
- Customer Segmentation (marketing groups)
- Document Categorization
- Image Segmentation
- Genomics (group similar genes)
- Anomaly Detection (outliers = separate cluster)

---

## ğŸ¯ K-MEANS CLUSTERING

### **Algorithm:**

```
1. Choose K (number of clusters)
2. Randomly initialize K centroids
3. Assign each point to nearest centroid
4. Update centroids = mean of assigned points
5. Repeat 3-4 until convergence
```

### **Visual Example:**

```
Initial:                After Iteration 1:      Converged:
   â—  â—                     â—  â—                  â— â—
  â— âŠ• â—                   â—  âŠ•  â—                â—âŠ•â—
   â—  â—                     â—  â—                  â— â—

  â— â—                      â— â—                     â—â—
 â— âŠ• â—                    â—  âŠ•  â—                 â—âŠ•â—
  â— â—                      â— â—                     â—â—

âŠ• = Centroid              Points move to         Stable clusters!
â— = Data point            nearest centroid
```

### **Advantages:**
âœ… Simple and fast
âœ… Scales well to large datasets
âœ… Works well with spherical clusters
âœ… Easy to interpret

### **Disadvantages:**
âŒ Must specify K beforehand
âŒ Sensitive to initialization
âŒ Assumes spherical clusters
âŒ Affected by outliers

### **Choosing Optimal K:**

**1. Elbow Method**
```python
# Plot Inertia (sum of squared distances) vs K
# Look for "elbow" in curve
Inertia
   |  â•²
   |   â•²___________
   |________________
     1  2  3  4  5  K
         â†‘ Elbow at K=3!
```

**2. Silhouette Score**
```python
# Measures how similar point is to own cluster vs other clusters
# Range: -1 (bad) to +1 (perfect)
# Choose K with highest average silhouette
```

**3. Domain Knowledge**
```python
# Sometimes K is known from business context
# E.g., Customer segments: Bronze, Silver, Gold = K=3
```

---

## ğŸŒ³ HIERARCHICAL CLUSTERING

### **Concept:**

Creates a **tree of clusters** (dendrogram) showing hierarchical relationships.

```
                    ALL DATA
                   /        \
              Cluster A    Cluster B
              /     \        /     \
            C1     C2      C3     C4
```

### **Two Approaches:**

**1. Agglomerative (Bottom-Up):**
```
Step 1: Each point is own cluster
Step 2: Merge closest clusters
Step 3: Repeat until 1 cluster remains
```

**2. Divisive (Top-Down):**
```
Step 1: All points in 1 cluster
Step 2: Split into smaller clusters
Step 3: Repeat until each point is own cluster
```

### **Linkage Methods:**

**Single Linkage:** Minimum distance between clusters
**Complete Linkage:** Maximum distance between clusters
**Average Linkage:** Average distance between all pairs
**Ward Linkage:** Minimize variance (most commonly used!)

### **Dendrogram:**
```
Height
   |           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   |      â”€â”€â”¬â”€â”€            |
   |    â”€â”€â”¬â”€â”˜    â”€â”€â”¬â”€â”€     |
   |  â”€â”€â”¬â”€â”˜     â”€â”€â”¬â”€â”˜      |
   |___â—___â—___â—___â—___â—___â—
      1   2   3   4   5   6

Cut at different heights â†’ different K!
```

### **Advantages:**
âœ… No need to specify K upfront
âœ… Produces dendrogram (interpretable)
âœ… Works with any distance metric
âœ… Deterministic (same result every run)

### **Disadvantages:**
âŒ Slow (O(nÂ³) time complexity)
âŒ Not suitable for large datasets
âŒ Sensitive to noise/outliers
âŒ Once merged, can't undo

---

## ğŸ”µ DBSCAN (Density-Based Clustering)

### **Concept:**

Clusters based on **density** - groups areas with high point concentration.

**Key Parameters:**
- **eps (Îµ):** Maximum distance between points to be neighbors
- **min_samples:** Minimum points to form dense region

### **Point Types:**

1. **Core Point:** Has â‰¥ min_samples neighbors within eps
2. **Border Point:** Within eps of core point, but not core itself
3. **Noise Point:** Not core, not within eps of core (outlier!)

```
        Core Points: â—
     Border Points: â—‹
      Noise Points: Ã—

    â—â”â”â—â”â”â—
    â”ƒ     â”ƒ
    â—     â—â”â”â—‹

    Ã—           Ã—

         â—â”â”â—
         â”ƒ  â”ƒ
         â—â”â”â—
```

### **Advantages:**
âœ… **No need to specify K!**
âœ… Can find arbitrarily shaped clusters
âœ… Robust to outliers (marks them as noise)
âœ… Works well with spatial data

### **Disadvantages:**
âŒ Sensitive to eps and min_samples
âŒ Struggles with varying densities
âŒ Not suitable for high-dimensional data
âŒ Difficult to interpret parameters

### **When to Use:**
- Geographic/spatial clustering
- Outlier detection
- Clusters with irregular shapes
- Don't know K beforehand

---

## ğŸ“‰ PCA (Principal Component Analysis)

### **What is PCA?**

**Dimensionality Reduction:** Transform high-dimensional data to lower dimensions while preserving most information.

```
Before:                    After:
10 features                2 features
(complex, hard to plot)    (simple, easy to plot)

Still captures 95% of variance!
```

### **How PCA Works:**

```
Step 1: Center data (subtract mean)
Step 2: Compute covariance matrix
Step 3: Find eigenvectors (principal components)
Step 4: Sort by eigenvalues (importance)
Step 5: Project data onto top K components
```

### **Visual Intuition:**

```
Original 2D Data:          PCA Finds New Axes:
      |                          â•± PC1
    â— | â—â—                     â—â•±â—â—
   â—â—â—|â—â—â—                   â—â—â•±â—â—â—
  â—â—â—â—|â—â—â—â—                 â—â—â•±â—â—â—â—
â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€ â†’           â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€ PC2
  â—â—â—â—|â—â—â—â—               â—â•±â—â—â—â—â—
   â—â—â—|â—â—â—                 â•± â—â—â—
    â— | â—â—                â•±  â—â—
      |

PC1 = Direction of maximum variance
PC2 = Orthogonal to PC1
```

### **Explained Variance:**
```
Component  Variance  Cumulative
PC1        45%       45%
PC2        30%       75%
PC3        15%       90%
PC4        7%        97%
PC5        3%        100%

â†’ Keep PC1-PC3 to retain 90% information!
```

### **Advantages:**
âœ… Reduces dimensionality (faster training!)
âœ… Removes multicollinearity
âœ… Noise reduction
âœ… Data visualization (2D/3D)
âœ… Unsupervised feature extraction

### **Disadvantages:**
âŒ Components hard to interpret
âŒ Assumes linear relationships
âŒ Sensitive to scaling (must standardize!)
âŒ May lose some information

### **Use Cases:**
- Visualize high-dimensional data
- Speed up machine learning
- Reduce storage/computation
- Remove correlated features
- Image compression

---

## ğŸ¨ t-SNE (t-Distributed Stochastic Neighbor Embedding)

### **What is t-SNE?**

**Non-linear dimensionality reduction** optimized for **visualization** (usually 2D or 3D).

### **PCA vs t-SNE:**

| Aspect | PCA | t-SNE |
|--------|-----|-------|
| **Speed** | Fast âš¡ | Slow ğŸŒ |
| **Method** | Linear | Non-linear |
| **Purpose** | Feature reduction | **Visualization only!** |
| **Preserves** | Variance | **Local structure** |
| **Deterministic** | Yes | No (random init) |

### **When to Use:**
âœ… Visualizing high-dimensional data (100+ features)
âœ… Exploring data structure
âœ… Identifying clusters visually
âœ… Publication-quality plots

âŒ **DON'T use for:**
- Machine learning features (use PCA!)
- Inference on new data
- Quantitative analysis

### **Key Parameters:**

**perplexity:** Balance between local and global structure (5-50)
**learning_rate:** Step size for optimization (10-1000)
**n_iter:** Number of iterations (250-5000)

---

## ğŸš¨ ANOMALY DETECTION

### **What are Anomalies?**

**Anomalies (Outliers):** Data points that significantly differ from normal patterns.

**Examples:**
- Fraudulent credit card transactions ğŸ’³
- Network intrusions ğŸ”’
- Defective products ğŸ­
- Unusual patient vitals ğŸ¥

### **Isolation Forest:**

**Concept:** Anomalies are easier to isolate (require fewer splits).

```
Normal Point:              Anomaly:
Many splits to isolate     Few splits!
      |                         |
   â”€â”€â”€â”¼â”€â”€â”€                  â”€â”€â”€â”€â”¼â”€â”€â”€ â—
  â—â—â— | â—â—â—                     |
   â—â—â—|â—â—â—
      |
```

**How it works:**
1. Randomly select feature and split value
2. Recursively partition data
3. Anomalies have **shorter path** to isolation
4. Score = average path length across trees

### **Advantages:**
âœ… Fast and scalable
âœ… Works in high dimensions
âœ… Few hyperparameters
âœ… Unsupervised (no labels needed!)

### **Use Cases:**
- Fraud detection
- Quality control
- System monitoring
- Medical diagnosis

---

## ğŸ“Š Clustering Evaluation Metrics

### **1. Silhouette Score**
```python
Score range: -1 to +1
+1: Perfect clustering
 0: Overlapping clusters
-1: Wrong clustering

Formula: (b - a) / max(a, b)
a = avg distance within cluster
b = avg distance to nearest cluster
```

### **2. Davies-Bouldin Index**
```python
Lower is better
Measures cluster separation vs compactness
```

### **3. Calinski-Harabasz Index**
```python
Higher is better
Ratio of between-cluster to within-cluster variance
```

### **âš ï¸ Warning:**
These metrics are **internal** (no ground truth needed) but:
- May not align with business goals
- Should be combined with domain knowledge
- Visual inspection still important!

---

## ğŸ¯ Module 19 Contents

### **19.1 K-Means Clustering**
- Implementation & visualization
- Elbow method for optimal K
- Silhouette analysis
- Customer segmentation example

### **19.2 Hierarchical Clustering**
- Agglomerative clustering
- Dendrogram visualization
- Linkage methods comparison
- Product categorization example

### **19.3 DBSCAN**
- Parameter tuning (eps, min_samples)
- Noise detection
- Geographic clustering example

### **19.4 PCA**
- Dimensionality reduction
- Explained variance
- Feature visualization
- Data compression example

### **19.5 t-SNE**
- High-dimensional visualization
- Parameter tuning
- Cluster visualization
- MNIST digit visualization

### **19.6 Anomaly Detection**
- Isolation Forest
- Anomaly scoring
- Fraud detection example

---

## ğŸš€ Quick Start

```bash
# Navigate to module
cd 05_Machine_Learning/19_Unsupervised_Learning

# Install dependencies
pip install -r requirements.txt

# Run clustering example
python 19_clustering_complete.py

# Run PCA example
python 19_pca_tsne_complete.py

# Run anomaly detection
python 19_anomaly_detection.py
```

---

## ğŸ’¡ Real-World Applications

### **1. Customer Segmentation** ğŸ›’
```
Cluster customers by:
- Purchase behavior
- Demographics
- Engagement level
â†’ Personalized marketing!
```

### **2. Image Segmentation** ğŸ–¼ï¸
```
Group similar pixels:
- Medical imaging (tumor detection)
- Satellite imagery (land use)
- Object recognition
```

### **3. Recommendation Systems** ğŸ“º
```
Cluster similar items/users:
- Netflix: group similar movies
- Spotify: group similar songs
- Amazon: product recommendations
```

### **4. Fraud Detection** ğŸ”’
```
Anomaly detection:
- Credit card fraud
- Insurance claims
- Network intrusions
```

### **5. Gene Expression Analysis** ğŸ§¬
```
Cluster genes with similar expression:
- Disease classification
- Drug discovery
- Personalized medicine
```

---

## ğŸ“ Best Practices

### **1. Always Standardize Data**
```python
# Clustering is distance-based!
# Features with different scales will dominate
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### **2. Visualize First**
```python
# Use PCA or t-SNE to see if clusters exist
# Don't force clustering on random data!
```

### **3. Try Multiple Methods**
```python
# Different algorithms for different data:
# - Spherical clusters â†’ K-Means
# - Hierarchical structure â†’ Hierarchical
# - Arbitrary shapes â†’ DBSCAN
```

### **4. Validate Results**
```python
# Use multiple metrics
# Check business relevance
# Iterate and refine
```

---

## ğŸ“š Comparison Table

| Method | Speed | Scalability | K Required | Cluster Shape | Outliers |
|--------|-------|-------------|------------|---------------|----------|
| **K-Means** | âš¡âš¡âš¡ | Excellent | Yes | Spherical | Sensitive |
| **Hierarchical** | ğŸŒ | Poor | No | Any | Sensitive |
| **DBSCAN** | âš¡âš¡ | Good | No | Any | **Robust** |
| **PCA** | âš¡âš¡âš¡ | Excellent | N/A | Linear | Sensitive |
| **t-SNE** | ğŸŒğŸŒ | Poor | N/A | Non-linear | Robust |

---

**Â© Muhammad Ketsar Ali Abi Wahid**

**Data Science Zero to Hero: Complete MLOps & Production ML Engineering**

**Module 19 - Unsupervised Learning Complete**

---

> "Unsupervised learning is like exploring a new city without a map - you discover hidden gems by wandering and observing patterns!" ğŸ—ºï¸âœ¨
